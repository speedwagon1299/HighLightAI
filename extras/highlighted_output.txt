**Low-Rank Adaptation for Convolutions: Efficient Fine-Tuning of Pretrained Vision Models Srihari Srinivasan, Morvin Prajapati January 202**
5
** Abstract This paper introduces a novel approach to fine-tune complex computer vision models like Depth Anything v2 (DAV2) for downstream tasks using LyCORIS Lo- CON, a low-rank adaptation method tailored for con- volutional neural networks**
. 
**By integrating only 150k additional parameters into DAV2’s decoder layer, this method enables binary human segmentation with performance on par with state-of-the-art models such as SAM, MaskFormer, and Seg2Former**
. 
**Extensive experiments on filtered subsets of the COCO and Im- ageNet datasets demonstrate that the proposed ap- proach achieves competitive mean Average Precision (mAP) scores while maintaining computational effi- ciency**
. 
**1 Introduction The rise of large-scale pre-trained models in com- puter vision has paved the way for significant ad- vancements across various tasks**
. 
**However, adapting such models to new domains or tasks often involves resource-intensive fine-tuning, which is impractical in resource-constrained environments**
. 
**This work addresses the challenge of efficient adap- tation by employing LyCORIS LoCON, a low-rank adaptation technique, to fine-tune the Depth Any- thing v2 (DAV2) model for binary human segmen- tation**
. With only 150k additional trainable parame- ters, LoCON achieves task-specific updates while pre- serving the pre-trained parameters of DAV2. 
**The contributions of this paper are as follows: •We demonstrate the ability of LyCORIS LoCON to fine-tune large vision models for downstream tasks with minimal additional parameters**
. 
**•We validate the proposed approach on the COCO and ImageNet human datasets, achieving segmentation performance comparable to SOTA models in mAP metrics.•We provide a modular and scalable architecture that maintains DAV2’s inference efficiency, mak- ing it applicable for real-time vision tasks**
. 
**2 Background The rapid growth of pre-trained models has driven the need for parameter-efficient fine-tuning meth- ods to address their computational and memory de- mands**
. 
**Low-Rank Adaptation (LoRA) achieves this by injecting trainable low-rank matrices into pre- trained Transformer layers, significantly reducing the number of trainable parameters and GPU memory re- quirements [1]**
. Building on this foundation, Weight- Decomposed Low-Rank Adaptation (DoRA) refines the process by decomposing pre-trained weights into magnitude and direction, enabling more efficient up- dates while maintaining training stability [2]. 
**To tackle memory constraints in large models, QLoRA introduces 4-bit quantization and memory- efficient innovations, allowing fine-tuning of massive models on hardware with limited resources [3]**
. Ex- tending LoRA’s efficiency, MoRA incorporates high- rank updates with square matrices, enhancing the model’s learning capacity for memory-intensive tasks without additional deployment complexity [4]. Recognizing the need for adaptive parameter al- location, AdaLoRA dynamically assigns parameter budgets based on importance scores, ensuring effi- cient fine-tuning while retaining task performance across various NLP tasks [5]. While LoRA-based methods have primarily focused on NLP, the Vision Transformer (ViT) demonstrates the efficacy of pure Transformer architectures for computer vision, out- performing traditional CNNs by applying attention mechanisms directly to image patches [6]. Expanding on the potential of Transformer-based models in vision, DINOv2 employs self-supervised learning on curated datasets, scaling model and data size to establish robust, general-purpose visual fea- tures [7]. Complementing this, the Segment Any- thing Model (SAM) introduces a promptable segmen- 1 tation framework, efficiently combining flexible en- coding and decoding components for real-time, scal- able segmentation tasks [8]. 
**To address SAM’s limitations in specialized do- mains, Convolution Meets LoRA (Conv-LoRA) in- tegrates convolutional biases into LoRA, enhancing SAM’s adaptability for domain-specific applications like medical imaging and remote sensing [9]**
. Building on this, Depth Anything V1 emphasizes large-scale dataset collection and auxiliary supervision, achiev- ing robust monocular depth estimation across diverse environments [10]. Improving upon its predecessor, Depth Anything V2 introduces synthetic training data and advanced teacher-student learning strategies, resulting in scal- able models with superior depth estimation capabil- ities [11]. Finally, LyCORIS LoCON incorporates convolutional adaptations with low-rank methods, fine-tuning the final decoder of Depth Anything V2 to optimize segmentation performance in domain- specific tasks [12]. 3 Mathematical Formulation of the Problem Statement Given an input RGB image I∈RH×W×3, the goal is to simultaneously predict: 1. A continuous depth map D∈RH×W, where each pixel value represents normalized depth in the range [0 ,1]. 2. A binary segmentation mask S∈ {0,1}H×W, in- dicating human (1) or background (0) pixels. Formally, the model computes: D, S =fθ(I) where fθrepresents the modified DAV2 model with parameters θ. The objective is to minimize the seg- mentation loss LS, given by: LS= Dice Loss( S, S∗) +λ·L1 Loss( S, S∗) where S∗is the ground truth segmentation mask, and λis a weighting factor. The depth estimation output Dis preserved from DAV2 without modification or additional loss. 4 Architecture Overview 4.1 Model Design The proposed architecture builds upon Depth Any- thing V2, retaining its DINOv2 encoder and depthhead while introducing modifications in the decoder to produce dual outputs: depth maps and binary hu- man segmentation masks. The overall architecture is shown in Figure 1. Figure 1: Overview of the modified DAV2 architec- ture. 4.2 Modified DPT Head Decoder The modified decoder incorporates LoCON modules for low-rank updates to convolutional layers, enabling segmentation alongside depth estimation. Figure 2 illustrates the architecture of the modified DPT head. Figure 2: Modified DPT head decoder with dual-path outputs. 4.3 Final Decoder Layers The final layers for both depth and segmentation paths are distinct instances of the same architectural design, as shown in Figure 3. This shared design ensures efficiency while preserving task-specific opti- mization. Figure 3: Final decoder architecture for depth and segmentation outputs. 2 5 Methodology 5.1 Preliminaries 5.1.1 Depth Anything V2 Depth Anything V2 (DAV2) represents a signifi- cant advancement in monocular depth estimation by leveraging a multi-stage training pipeline [11]. The methodology is as follows: 1.Training a Teacher Model : The pipeline be- gins with training a large-scale teacher model based on the DINOv2-Giant backbone. This model is trained exclusively on a dataset of approximately 595,000 synthetic images to establish a robust under- standing of depth estimation in a controlled synthetic environment [7]. 2.Pseudo-Label Generation : The trained teacher model is then utilized to generate high-quality pseudo-depth labels for over 62 million unlabeled real-world images. This step is crucial to bridge the domain gap between synthetic and real-world datasets and enhance the model’s generalization ca- pability. 3.Training the Student Model : The final stage involves training student models on the pseudo- labeled real-world dataset. This step ensures that the model learns finer depth estimation features and adapts effectively to real-world scenarios. 4.DPT Head Decoder : The DAV2 model employs a robust DPT head decoder composed of ResNet-based convolutional layers. This decoder is designed to effectively capture multi-scale features and refine depth predictions by leveraging residual connections. The ResNet convolutions in the decoder provide a strong capability for spatial feature integra- tion, enhancing depth estimation accuracy [13]. The combination of synthetic-to-real transfer learning, large-scale pseudo-labeling, and a robust DINOv2 backbone makes DAV2 a state-of-the-art ap- proach for depth estimation tasks. 5.1.2 Overview of Low Rank Adaptation (LoRA) Low-Rank Adaptation (LoRA) builds on the premise that fine-tuning updates exhibit a low intrinsic rank, enabling efficient adaptation of pre-trained models [1]. LoRA introduces a pair of low-rank matrices to approximate weight updates, significantly reduc- ing the number of trainable parameters while main- taining the expressiveness of the model. Given a pre-trained weight matrix W0∈Rd×k, LoRA models the weight update ∆ W∈Rd×kas the product of two low-rank matrices, B∈Rd×rand A∈Rr×k, where r≪min(d, k). The adapted weightW′is computed as: W′=W0+ ∆W=W0+BA where W0remains frozen during training, and only the parameters of BandAare updated. The initial- ization ensures Bis set to zero and Ais initialized using the Kaiming distribution, making ∆ Winitially zero. This setup allows the model to leverage pre- trained knowledge while introducing minimal over- head. 
**5.1.3 LoCON: Low-Rank Adaptation for Convolutions LoCON extends the principles of Low-Rank Adap- tation (LoRA) to convolutional layers, enabling effi- cient fine-tuning of pre-trained convolutional neural networks (CNNs) [1]**
. For a convolutional weight tensor W0∈ RCout×Cin×H×W, LoCON introduces two trainable low-rank matrices, B ∈ Rr×Cin×H×WandA ∈ RCout×r×1×1, where r≪min(Cout, Cin). The weight update is modeled as: W′=W0+A · B where ·denotes matrix multiplication. Here, Bre- duces the input dimensionality to a smaller rank r, andArestores the rank to match the output chan- nels. By freezing W0during training and updating only the low-rank matrices, LoCON reduces the number of trainable parameters while retaining the expressive power of the original model. 
**5.2 Modification to Decoder Architec- ture 5.2.1 Integration of LoCON Adapter Layers into DPT Decoder Convolutions The integration of Low-Rank Adaptation for Con- volutions (LoCON) into the Depth Anything V2 (DAV2) architecture enhances the fine-tuning effi- ciency of the model by introducing lightweight and adaptive updates to convolutional layers**
. These Lo- CON modules are embedded into the DPT head’s convolutional layers, enabling task-specific adapta- tion while preserving the pre-trained knowledge of the base model. LoCON decomposes the weight updates of each convolutional layer into two low-rank matrices. Dur- ing inference, they are multiplied and the result is added to the existing pre-trained convolutional weight matrices. Two paths emerge from this, one which carries the convolution operation carried out through the original weight matrices to produce the 3 unmodified depth estimate and the other path holds the convolution operation carried through the up- dated weight matrices which will become the paral- lelly obtained binary human segmentation mask. 5.2.2 Fusion of LoCON Output with Depth Features via Upsampling Our architecture incorporates LoCON-enhanced con- volutional outputs into the feature refinement process by merging these outputs with depth features in a hierarchical manner. This process ensures efficient feature fusion across multiple scales, contributing to robust depth prediction and segmentation results. LoCON outputs from convolutional layers are first upsampled to match the spatial resolution of the re- finement path output by the Residual Network of CNNs. The upsampled LoCON features are merged with the intermediate depth features produced by the decoder using element-wise addition. This hierarchi- cal fusion is performed at multiple levels, progres- sively integrating finer details and context-aware in- formation as the features are propagated through the decoder. The fusion process can be described as: F′=U(FLoCON ) +FDecoder where FLoCON represents the output features from LoCON-enhanced convolutions, FDecoder denotes the intermediate features from the decoder, and U(·) is the bilinear upsampling operation. The merged fea- tures F′are then passed to subsequent refinement layers for further processing. 5.2.3 Dual-Path Decoder Architecture The model incorporates a dual-path decoder architec- ture, designed to simultaneously produce depth and segmentation outputs from a shared feature represen- tation. The decoder operates hierarchically, progres- sively refining features from the encoder through a series of convolutional and upsampling layers. These refined features are then split into two independent paths: •Depth Path: Outputs a continuous depth map, with a single-channel prediction representing the depth values normalized using a sigmoid activa- tion function. •Segmentation Path: Produces a binary mask, highlighting regions of interest (e.g., human seg- mentation), also normalized using a sigmoid ac- tivation function. The final layers of the decoder for both the depth and segmentation paths are distinct instances of thesame architectural design, which is adapted from the original Depth Anything V2 (DAV2) decoder output layer. The process for generating each output can be expressed as: O=σ(Conv(Interpolate(Conv( F)))) where Frepresents the input feature map, Conv( ·) denotes a convolution operation, Interpolate( ·) rep- resents bilinear upsampling to the target resolution, andσ(·) is the sigmoid activation function. The final layers of the decoder for both the depth and segmentation paths are distinct instances of the same architectural design, which is adapted from the original Depth Anything V2 (DAV2) decoder output layer. The design initially optimized for depth pre- diction proves effective for segmentation tasks as well, maintaining architectural efficiency. 
**5.3 Training and Inference 5.3.1 Training The model is trained on a subset of the COCO dataset containing 30,000 images with the ”person” tag**
. 
**Ground truth segmentation masks are derived using SAM with an IoU threshold of 0.8**
. 
**Key train- ing details are as follows: •Optimizer: AdamW with a learning rate of 0.00005**
. 
**•Batch Size: 64**
. 
**•Epochs: 30**
. •Augmentations: Horizontal flip with a proba- bility of 0.5. •Loss Function: Dice + L1 Loss for segmenta- tion. 
**5.3.2 Inference During inference, the model processes RGB images of any resolution, preserving aspect ratio and resolu- tion using DAV2’s preprocessing pipeline**
. Depth and segmentation outputs are generated concurrently, en- suring efficiency. 
**6 Experiments 6.1 Datasets Filtered subsets of COCO and ImageNet were used, selecting images with significant human presence based on pixel thresholds**
. 
**6.2 Metrics and Baselines Performance was evaluated using mAP, comparing against SAM, MaskFormer, and Seg2Former**
. 
**4 6.3 Results The proposed model achieved competitive mAP and mIoU scores, with significantly lower computational overhead**
. The comparative results are presented in Table 1. Model Mean AP Mean IoU Mask2Former 0.9076 0.8162 SAM 0.8877 0.7508 segformer 0.8769 0.7489 ours 0.8969 0.7917 Table 1: Comparative Results of Mean Average Precision (mAP) and Mean Intersection over Union (mIoU) 7 Conclusion The proposed model demonstrates performance com- parable to that of state-of-the-art segmentation mod- els, achieving competitive mAP and mIoU scores. The model also retains the ability to output depth masks identical to that of the depth-anything model. References [1] Edward Hu, Yelong Shen, Phillip Wallis, et al. 
**Low-rank adaptation (lora)**
. arXiv preprint arXiv:2106.09685 , 2021. [2] Anonymous. Weight-decomposed low- rank adaptation (dora). arXiv preprint arXiv:2402.09353 , 2024. [3] Tim Dettmers et al. 
**Qlora: Efficient fine- tuning with quantization**
. arXiv preprint arXiv:2305.14314 , 2023. [4] Anonymous. Mora: High-rank updating for parameter-efficient fine-tuning. arXiv preprint arXiv:2405.12130 , 2024. [5] Anonymous. Adalora: Adaptive low-rank fine- tuning for large language models. arXiv preprint arXiv:2303.10512 , 2023. [6] Anonymous. Vision transformer (vit): A pure transformer for vision tasks. arXiv preprint arXiv:2010.11929 , 2020. [7] Anonymous. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 , 2023.[8] Anonymous. Segment anything model (sam): A promptable segmentation framework. arXiv preprint arXiv:2304.02643 , 2023. [9] Anonymous. 
**Convolution meets lora**
. arXiv preprint arXiv:2401.17868 , 2024. [10] Anonymous. Depth anything v1: A robust so- lution for monocular depth estimation. arXiv preprint arXiv:2401.10891 , 2024. [11] Anonymous. 
**Depth anything v2**
. arXiv preprint arXiv:2406.09414 , 2024. [12] Anonymous. 
**Lycoris - locon**
. OpenReview preprint , 2024. [13] Vladlen Koltun Intel Labs Rene Ranftl, Alexey Bochkovskiy. Vision transform- ers for dense prediction. arXiv preprint arXiv:2103.13413 , 2021. 5